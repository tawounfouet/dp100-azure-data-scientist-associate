{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer un composant\n",
    "Un composant se compose de trois parties :\n",
    "\n",
    "- Métadonnées : inclut le nom, la version, etc. du composant.\n",
    "- Interface : inclut les paramètres d’entrée attendus (comme un jeu de données ou un hyperparamètre) et la sortie attendue (comme des métriques et des artefacts).\n",
    "- Commande, code et environnement : spécifie comment exécuter le code.\n",
    "\n",
    "Pour créer un composant, vous avez besoin de deux fichiers :\n",
    "\n",
    "Script qui contient le workflow que vous souhaitez exécuter.\n",
    "Fichier YAML définissant les métadonnées, l’interface et la commande, le code et l’environnement du composant.\n",
    "Vous pouvez créer le fichier YAML ou utiliser la fonction command_component() en tant qu’élément décoratif pour créer le fichier YAML.\n",
    "\n",
    "Nous allons maintenant nous concentrer sur la création d’un fichier YAML pour créer un composant. Vous pouvez également en savoir plus en consultant le guide pratique pour créer des composants à l’aide de command_component(): https://docs.microsoft.com/fr-fr/azure/machine-learning/how-to-create-your-first-pipeline#build-a-pipeline-with-a-python-script\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez par exemple disposer d’un script Python prep.py qui prépare les données en supprimant les valeurs manquantes et en normalisant les données : \n",
    "\n",
    "```python\n",
    "# import libraries\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# setup arg parser\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# add arguments\n",
    "parser.add_argument(\"--input_data\", dest='input_data',\n",
    "                    type=str)\n",
    "parser.add_argument(\"--output_data\", dest='output_data',\n",
    "                    type=str)\n",
    "\n",
    "# parse args\n",
    "args = parser.parse_args()\n",
    "\n",
    "# read the data\n",
    "df = pd.read_csv(args.input_data)\n",
    "\n",
    "# remove missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# normalize the data    \n",
    "scaler = MinMaxScaler()\n",
    "num_cols = ['feature1','feature2','feature3','feature4']\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "# save the data as a csv\n",
    "output_df = df.to_csv(\n",
    "    (Path(args.output_data) / \"prepped-data.csv\"), \n",
    "    index = False\n",
    ")\n",
    "```\n",
    "\n",
    "Pour créer un composant pour le script prep.py, vous avez besoin d’un fichier YAML prep.yml :\n",
    "```yaml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: prep_data\n",
    "display_name: Prepare training data\n",
    "version: 1\n",
    "type: command\n",
    "inputs:\n",
    "  input_data: \n",
    "    type: uri_file\n",
    "outputs:\n",
    "  output_data:\n",
    "    type: uri_file\n",
    "code: ./src\n",
    "environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
    "command: >-\n",
    "  python prep.py \n",
    "  --input_data ${{inputs.input_data}}\n",
    "  --output_data ${{outputs.output_data}}\n",
    "```\n",
    "\n",
    "Notez que le fichier YAML fait référence au script prep.py qui est stocké dans le dossier src. Vous pouvez charger le composant avec le code suivant :\n",
    "    \n",
    "```python\n",
    "from azure.ai.ml import load_component\n",
    "parent_dir = \"\"\n",
    "\n",
    "loaded_component_prep = load_component(source=parent_dir + \"./prep.yml\")\n",
    "```\n",
    "\n",
    "\n",
    "Une fois que vous avez chargé le composant, vous pouvez l’utiliser dans un pipeline ou l’inscrire.\n",
    "\n",
    "### Inscrire un component\n",
    "Pour utiliser des composants dans un pipeline, vous avez besoin du script et du fichier YAML. Pour que les composants soient accessibles aux autres utilisateurs de l’espace de travail, vous pouvez également inscrire des composants dans l’espace de travail Azure Machine Learning.\n",
    "\n",
    "Vous pouvez inscrire un composant avec le code suivant :\n",
    "```bash\n",
    "prep = ml_client.components.create_or_update(prepare_data_component)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créer un pipeline\n",
    "\n",
    "Un pipeline Azure Machine Learning est défini dans un fichier YAML. Le fichier YAML comprend le nom du travail de pipeline, les entrées, les sorties et les paramètres.\n",
    "\n",
    "Vous pouvez créer le fichier YAML ou utiliser la fonction @pipeline() pour créer le fichier YAML.\n",
    "\n",
    "Si vous souhaitez par exemple créer un pipeline qui prépare d’abord les données, puis effectue l’apprentissage du modèle, vous pouvez utiliser le code suivant :\n",
    "```python\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "\n",
    "@pipeline()\n",
    "def pipeline_function_name(pipeline_job_input):\n",
    "    prep_data = loaded_component_prep(input_data=pipeline_job_input)\n",
    "    train_model = loaded_component_train(training_data=prep_data.outputs.output_data)\n",
    "\n",
    "    return {\n",
    "        \"pipeline_job_transformed_data\": prep_data.outputs.output_data,\n",
    "        \"pipeline_job_trained_model\": train_model.outputs.model_output,\n",
    "    }\n",
    "```\n",
    "\n",
    "Pour transmettre une ressource de données inscrite comme entrée du travail de pipeline, vous pouvez appeler la fonction que vous avez créée avec la ressource de données comme entrée :\n",
    "```python\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "pipeline_job = pipeline_function_name(\n",
    "    Input(type=AssetTypes.URI_FILE, \n",
    "    path=\"azureml:data:1\"\n",
    "))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
